"""
True Streaming TTS for IndexTTS2

This module implements token-level streaming by intercepting mel tokens
as they are generated by the GPT model, processing them in chunks through
S2Mel and BigVGAN to yield audio with minimal latency.

Key optimizations:
1. Token-level streaming using HuggingFace's BaseStreamer interface
2. Chunked audio processing (every N mel tokens -> audio chunk)
3. Overlap of GPT generation with S2Mel/BigVGAN processing using threading
4. Async pipeline: GPT generates while previous chunk is being synthesized

For higher quality streaming, see streaming_v2.py which provides:
- Sentence-level streaming (better prosodic coherence)
- Progressive mel context (smoother transitions)
- Overlap synthesis (mel-domain blending)
"""

from __future__ import annotations

import threading
import queue
import time
import concurrent.futures
from typing import Generator, Optional, Callable, Any, List, Tuple
from dataclasses import dataclass, field
from enum import Enum

import torch
import torch.nn.functional as F
from transformers.generation.streamers import BaseStreamer


class StreamingQuality(Enum):
    """Streaming quality presets."""
    # Fastest TTFA, lowest quality - original chunk-by-chunk
    FAST = "fast"
    # Balanced - larger chunks, better crossfade
    BALANCED = "balanced"
    # High quality - sentence-level streaming (uses V2 backend)
    HIGH = "high"


@dataclass
class StreamingConfig:
    """Configuration for streaming TTS."""
    # Quality preset (overrides individual settings if set)
    quality: Optional[StreamingQuality] = None
    
    # Minimum mel tokens before first audio chunk (lower = faster first audio)
    min_chunk_tokens: int = 20
    # Tokens to accumulate after first chunk (can be larger for efficiency)
    chunk_tokens: int = 40
    # Maximum tokens before forcing a chunk (prevents long waits)
    max_chunk_tokens: int = 80
    # Diffusion steps for S2Mel (lower = faster but lower quality)
    diffusion_steps: int = 15
    # Diffusion steps for first chunk (lower for faster TTFA)
    first_chunk_diffusion_steps: int = 8
    # CFM inference rate
    inference_cfg_rate: float = 0.7
    # Whether to use threading for pipeline processing
    # NOTE: Threading is disabled by default due to CUDA single-GPU limitations
    use_threading: bool = False
    # Whether to synthesize during generation (blocks generation but yields faster)
    synthesize_during_generation: bool = True
    # Crossfade settings for smooth chunk transitions
    crossfade_samples: int = 1024  # Samples to overlap between chunks
    use_crossfade: bool = True  # Enable crossfading for smoother audio
    # Use raised cosine crossfade (smoother than linear)
    use_cosine_crossfade: bool = True
    # Verbose logging
    verbose: bool = False
    
    def __post_init__(self):
        """Apply quality preset if specified."""
        if self.quality is not None:
            self._apply_quality_preset()
    
    def _apply_quality_preset(self):
        """Apply settings from quality preset."""
        if self.quality == StreamingQuality.FAST:
            self.min_chunk_tokens = 15
            self.chunk_tokens = 35
            self.max_chunk_tokens = 70
            self.diffusion_steps = 12
            self.first_chunk_diffusion_steps = 6
            self.crossfade_samples = 512
        elif self.quality == StreamingQuality.BALANCED:
            self.min_chunk_tokens = 25
            self.chunk_tokens = 50
            self.max_chunk_tokens = 100
            self.diffusion_steps = 18
            self.first_chunk_diffusion_steps = 10
            self.crossfade_samples = 2048
            self.use_cosine_crossfade = True
        elif self.quality == StreamingQuality.HIGH:
            # HIGH quality uses V2 sentence-level streaming
            # These settings are fallbacks if V2 is not available
            self.min_chunk_tokens = 40
            self.chunk_tokens = 80
            self.max_chunk_tokens = 150
            self.diffusion_steps = 25
            self.first_chunk_diffusion_steps = 15
            self.crossfade_samples = 4096
            self.use_cosine_crossfade = True


class AudioSynthesizer:
    """
    Background thread worker for audio synthesis.
    
    This class handles the S2Mel + BigVGAN pipeline in a separate thread,
    allowing GPT generation to continue while audio is being synthesized.
    """
    
    def __init__(
        self,
        tts: 'IndexTTS2',
        config: StreamingConfig,
        spk_cond_emb: torch.Tensor,
        emo_vec: torch.Tensor,
        text_tokens: torch.Tensor,
        speech_conditioning_latent: torch.Tensor,
        style: torch.Tensor,
        prompt_condition: torch.Tensor,
        ref_mel: torch.Tensor,
    ):
        self.tts = tts
        self.config = config
        self.spk_cond_emb = spk_cond_emb
        self.emo_vec = emo_vec
        self.text_tokens = text_tokens
        self.speech_conditioning_latent = speech_conditioning_latent
        self.style = style
        self.prompt_condition = prompt_condition
        self.ref_mel = ref_mel
        
        # Thread-safe queues
        self.input_queue: queue.Queue[Tuple[List[int], bool]] = queue.Queue()
        self.output_queue: queue.Queue[torch.Tensor] = queue.Queue()
        
        # Control
        self.should_stop = False
        self.worker_thread: Optional[threading.Thread] = None
        self.is_running = False
        
    def start(self):
        """Start the background synthesis thread."""
        if self.is_running:
            return
        self.should_stop = False
        self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
        self.worker_thread.start()
        self.is_running = True
        
    def stop(self):
        """Stop the background synthesis thread."""
        self.should_stop = True
        # Send sentinel to unblock queue
        self.input_queue.put(([], True))
        if self.worker_thread is not None:
            self.worker_thread.join(timeout=5.0)
        self.is_running = False
        
    def submit(self, tokens: List[int], is_final: bool = False):
        """Submit tokens for synthesis in background."""
        self.input_queue.put((tokens, is_final))
        
    def get_audio(self, timeout: float = 0.1) -> Optional[torch.Tensor]:
        """Get synthesized audio from output queue (non-blocking)."""
        try:
            return self.output_queue.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def get_all_audio(self) -> List[torch.Tensor]:
        """Get all available synthesized audio chunks."""
        chunks = []
        while not self.output_queue.empty():
            try:
                chunks.append(self.output_queue.get_nowait())
            except queue.Empty:
                break
        return chunks
        
    def _worker_loop(self):
        """Background thread loop for audio synthesis."""
        device = self.spk_cond_emb.device
        
        while not self.should_stop:
            try:
                # Wait for tokens with timeout
                tokens, is_final = self.input_queue.get(timeout=0.1)
            except queue.Empty:
                continue
                
            if not tokens:
                if is_final:
                    break
                continue
            
            try:
                # Synthesize audio from tokens
                wav = self._synthesize(tokens)
                self.output_queue.put(wav)
            except Exception as e:
                if self.config.verbose:
                    print(f"  [Synthesizer] Error: {e}")
                continue
    
    def _synthesize(self, chunk_tokens: List[int]) -> torch.Tensor:
        """Synthesize audio from mel tokens."""
        device = self.spk_cond_emb.device
        
        # Convert tokens to tensor
        codes = torch.tensor([chunk_tokens], dtype=torch.long, device=device)
        code_lens = torch.tensor([len(chunk_tokens)], device=device)
        
        with torch.no_grad():
            dtype = self.tts.dtype
            use_autocast = dtype is not None and device.type == 'cuda'
            
            with torch.amp.autocast(device.type, enabled=use_autocast, dtype=dtype or torch.float32):
                # GPT forward pass for latent
                use_speed = torch.zeros(1, dtype=torch.long, device=device)
                
                latent = self.tts.gpt(
                    self.speech_conditioning_latent,
                    self.text_tokens,
                    torch.tensor([self.text_tokens.shape[-1]], device=device),
                    codes,
                    code_lens,
                    self.spk_cond_emb,
                    cond_mel_lengths=torch.tensor([self.spk_cond_emb.shape[1]], device=device),
                    emo_cond_mel_lengths=torch.tensor([self.spk_cond_emb.shape[1]], device=device),
                    emo_vec=self.emo_vec,
                    use_speed=use_speed,
                )
                
                # S2Mel stage
                latent = self.tts.s2mel.models['gpt_layer'](latent)
                S_infer = self.tts.semantic_codec.quantizer.vq2emb(codes.unsqueeze(1))
                S_infer = S_infer.transpose(1, 2)
                S_infer = S_infer + latent
                
                target_lengths = (code_lens * 1.72).long()
                
                cond = self.tts.s2mel.models['length_regulator'](
                    S_infer, ylens=target_lengths, n_quantizers=3, f0=None
                )[0]
                
                cat_condition = torch.cat([self.prompt_condition, cond], dim=1)
                
                # Diffusion
                vc_target = self.tts.s2mel.models['cfm'].inference(
                    cat_condition,
                    torch.LongTensor([cat_condition.size(1)]).to(device),
                    self.ref_mel,
                    self.style,
                    None,
                    self.config.diffusion_steps,
                    inference_cfg_rate=self.config.inference_cfg_rate
                )
                vc_target = vc_target[:, :, self.ref_mel.size(-1):]
                
                # BigVGAN
                wav = self.tts.bigvgan(vc_target.float()).squeeze()
        
        return wav.cpu().unsqueeze(0)


class MelTokenStreamer(BaseStreamer):
    """
    Custom streamer that intercepts mel tokens from GPT generation
    and triggers audio synthesis in chunks.
    
    This implements HuggingFace's BaseStreamer interface to receive
    tokens as they are generated, enabling true streaming synthesis.
    
    When threading is enabled, audio synthesis runs in a background thread
    to overlap with GPT generation.
    """
    
    def __init__(
        self,
        tts: 'IndexTTS2',
        config: StreamingConfig,
        # Pre-computed conditioning data
        spk_cond_emb: torch.Tensor,
        emo_cond_emb: torch.Tensor,  # Emotion conditioning embedding (separate tensor)
        emo_vec: torch.Tensor,
        text_tokens: torch.Tensor,
        speech_conditioning_latent: torch.Tensor,
        style: torch.Tensor,
        prompt_condition: torch.Tensor,
        ref_mel: torch.Tensor,
        # Callback for audio chunks
        on_audio_chunk: Optional[Callable[[torch.Tensor], None]] = None,
    ):
        self.tts = tts
        self.config = config
        self.spk_cond_emb = spk_cond_emb
        self.emo_cond_emb = emo_cond_emb
        self.emo_vec = emo_vec
        self.text_tokens = text_tokens
        self.speech_conditioning_latent = speech_conditioning_latent
        self.style = style
        self.prompt_condition = prompt_condition
        self.ref_mel = ref_mel
        self.on_audio_chunk = on_audio_chunk
        
        # Token accumulation buffer
        self.token_buffer: List[int] = []
        self.all_generated_tokens: List[int] = []
        self.pending_chunks: List[List[int]] = []  # Chunks to synthesize after generation
        self.is_first_chunk = True
        self.chunk_count = 0
        self.audio_chunks: List[torch.Tensor] = []
        
        # Crossfade state for smooth transitions
        self.previous_chunk_tail: Optional[torch.Tensor] = None  # Last N samples of previous chunk
        
        # Timing stats
        self.first_token_time: Optional[float] = None
        self.first_audio_time: Optional[float] = None
        self.start_time = time.perf_counter()
        
    def put(self, value: torch.Tensor):
        """
        Called by HuggingFace's generate() for each new token.
        Accumulates tokens and records chunk boundaries.
        
        NOTE: We do NOT synthesize during generation to avoid CUDA conflicts.
        Synthesis happens after generation completes, using recorded chunk boundaries.
        """
        if self.first_token_time is None:
            self.first_token_time = time.perf_counter()
            if self.config.verbose:
                print(f"  [Stream] First token at {self.first_token_time - self.start_time:.3f}s")
        
        # value is [batch_size] tensor of new tokens
        if value.dim() == 0:
            value = value.unsqueeze(0)
        
        new_tokens = value.squeeze().tolist()
        if isinstance(new_tokens, int):
            new_tokens = [new_tokens]
            
        # Get special token IDs
        stop_mel_token = self.tts.stop_mel_token
        start_mel_token = self.tts.gpt.start_mel_token
        
        for token in new_tokens:
            # Skip special tokens - they are not valid mel codes for synthesis
            # start_mel_token (8193) and stop_mel_token (8194) are outside codebook range
            if token == stop_mel_token:
                # Mark remaining tokens as final chunk
                if self.token_buffer:
                    self._mark_chunk_boundary(is_final=True)
                return
            if token == start_mel_token:
                # Skip BOS token - it's not a valid mel code
                continue
            self.token_buffer.append(token)
            self.all_generated_tokens.append(token)
        
        # Determine if we should mark a chunk boundary
        buffer_len = len(self.token_buffer)
        
        # First chunk: use smaller threshold for faster time-to-first-audio
        if self.is_first_chunk:
            threshold = self.config.min_chunk_tokens
        else:
            threshold = self.config.chunk_tokens
            
        # Mark chunk boundary when threshold reached
        if buffer_len >= self.config.max_chunk_tokens:
            self._mark_chunk_boundary()
        elif buffer_len >= threshold:
            self._mark_chunk_boundary()
    
    def _mark_chunk_boundary(self, is_final: bool = False):
        """Mark current tokens as a chunk boundary and optionally synthesize immediately."""
        if not self.token_buffer:
            return
            
        chunk_tokens = self.token_buffer.copy()
        self.token_buffer = []
        self.chunk_count += 1
        was_first_chunk = self.is_first_chunk
        self.is_first_chunk = False
        
        # Store chunk for later synthesis (if not synthesizing during generation)
        self.pending_chunks.append(chunk_tokens)
        
        if self.config.verbose:
            print(f"  [Stream] Marked chunk {self.chunk_count}: {len(chunk_tokens)} tokens")
        
        # If configured to synthesize during generation, do it now
        if self.config.synthesize_during_generation:
            wav = self._synthesize_chunk(chunk_tokens, is_first=was_first_chunk, is_final=is_final)
            if wav is not None:
                self.audio_chunks.append(wav)
                if self.first_audio_time is None:
                    self.first_audio_time = time.perf_counter()
                    if self.config.verbose:
                        ttfa = self.first_audio_time - self.start_time
                        print(f"  [Stream] FIRST AUDIO at {ttfa:.3f}s!")
                if self.on_audio_chunk is not None:
                    self.on_audio_chunk(wav)
    
    def _synthesize_chunk(self, chunk_tokens: List[int], is_first: bool = False, is_final: bool = False) -> Optional[torch.Tensor]:
        """Synthesize audio from a chunk of tokens."""
        if not chunk_tokens:
            return None
            
        device = self.spk_cond_emb.device
        codes = torch.tensor([chunk_tokens], dtype=torch.long, device=device)
        code_lens = torch.tensor([len(chunk_tokens)], device=device)
        
        # Use fewer diffusion steps for first chunk (faster TTFA) and middle chunks
        if is_first:
            diffusion_steps = self.config.first_chunk_diffusion_steps
        elif is_final:
            diffusion_steps = self.config.diffusion_steps
        else:
            diffusion_steps = max(8, self.config.diffusion_steps - 5)
        
        try:
            with torch.no_grad():
                use_autocast = self.tts.dtype is not None and device.type == 'cuda'
                with torch.amp.autocast(device.type, enabled=use_autocast, dtype=self.tts.dtype or torch.float32):
                    # GPT forward pass for latent
                    use_speed = torch.zeros(1, device=device, dtype=torch.long)
                    
                    latent = self.tts.gpt(
                        self.speech_conditioning_latent,
                        self.text_tokens,
                        torch.tensor([self.text_tokens.shape[-1]], device=device),
                        codes,
                        code_lens,
                        self.spk_cond_emb,
                        cond_mel_lengths=torch.tensor([self.spk_cond_emb.shape[1]], device=device),
                        emo_cond_mel_lengths=torch.tensor([self.spk_cond_emb.shape[1]], device=device),
                        emo_vec=self.emo_vec.squeeze(1) if self.emo_vec.dim() == 3 else self.emo_vec,
                        use_speed=use_speed,
                    )
                    
                    # S2Mel stage
                    latent = self.tts.s2mel.models['gpt_layer'](latent)
                    S_infer = self.tts.semantic_codec.quantizer.vq2emb(codes.unsqueeze(1))
                    S_infer = S_infer.transpose(1, 2)
                    S_infer = S_infer + latent
                    target_lengths = (code_lens * 1.72).long()
                    
                    cond = self.tts.s2mel.models['length_regulator'](
                        S_infer, ylens=target_lengths, n_quantizers=3, f0=None
                    )[0]
                    
                    cat_condition = torch.cat([self.prompt_condition, cond], dim=1)
                    
                    vc_target = self.tts.s2mel.models['cfm'].inference(
                        cat_condition,
                        torch.LongTensor([cat_condition.size(1)]).to(device),
                        self.ref_mel,
                        self.style,
                        None,
                        diffusion_steps,
                        inference_cfg_rate=self.config.inference_cfg_rate
                    )
                    vc_target = vc_target[:, :, self.ref_mel.size(-1):]
                    
                    # BigVGAN vocoding
                    wav = self.tts.bigvgan(vc_target.float()).squeeze()
            
            wav = torch.clamp(32767 * wav, -32767.0, 32767.0).cpu()
            if wav.dim() == 1:
                wav = wav.unsqueeze(0)
            
            # Apply crossfade with previous chunk for smooth transitions
            if self.config.use_crossfade and self.previous_chunk_tail is not None:
                wav = self._apply_crossfade(wav)
            
            # For non-final chunks: save tail for blending and TRIM it from output
            # This prevents echo/bleeding - the tail is only heard in the blend, not twice
            if self.config.use_crossfade and not is_final:
                crossfade_samples = self.config.crossfade_samples
                if wav.shape[-1] > crossfade_samples * 2:  # Only if chunk is long enough
                    # Store tail for blending into next chunk
                    self.previous_chunk_tail = wav[:, -crossfade_samples:].clone()
                    # TRIM tail from current output - it will appear in next chunk's blend
                    wav = wav[:, :-crossfade_samples]
                else:
                    self.previous_chunk_tail = None
            else:
                # Final chunk - output everything, no tail to store
                self.previous_chunk_tail = None
            
            return wav
        except Exception as e:
            if self.config.verbose:
                print(f"  [Stream] Synthesis error: {e}")
            return None
    
    def _apply_crossfade(self, wav: torch.Tensor) -> torch.Tensor:
        """
        Apply crossfade between previous chunk tail and current chunk head.
        
        This creates smooth transitions by:
        1. Fading out the end of the previous chunk
        2. Fading in the start of the current chunk
        3. Overlapping and blending them together
        
        Supports both linear and raised cosine crossfade curves.
        Raised cosine provides smoother transitions with less audible artifacts.
        
        Args:
            wav: Current chunk audio tensor (1, samples)
            
        Returns:
            Audio with crossfaded beginning (may be slightly shorter due to overlap)
        """
        if self.previous_chunk_tail is None:
            return wav
        
        crossfade_samples = min(
            self.config.crossfade_samples,
            self.previous_chunk_tail.shape[-1],
            wav.shape[-1]
        )
        
        if crossfade_samples < 32:  # Too short to be meaningful
            return wav
        
        # Create fade curves - use raised cosine for smoother transitions
        if getattr(self.config, 'use_cosine_crossfade', False):
            # Raised cosine crossfade (smoother, less audible artifacts)
            t = torch.linspace(0, 1, crossfade_samples, device=wav.device, dtype=wav.dtype)
            fade_in = 0.5 * (1 - torch.cos(torch.pi * t))
            fade_out = 0.5 * (1 + torch.cos(torch.pi * t))
        else:
            # Linear crossfade (original behavior)
            fade_in = torch.linspace(0.0, 1.0, crossfade_samples, device=wav.device, dtype=wav.dtype)
            fade_out = torch.linspace(1.0, 0.0, crossfade_samples, device=wav.device, dtype=wav.dtype)
        
        # Get the overlapping regions
        prev_tail = self.previous_chunk_tail[:, -crossfade_samples:]  # End of previous chunk
        curr_head = wav[:, :crossfade_samples]  # Start of current chunk
        
        # Apply fades and blend
        blended = (prev_tail * fade_out.unsqueeze(0)) + (curr_head * fade_in.unsqueeze(0))
        
        # Create output: blended region + rest of current chunk
        if wav.shape[-1] > crossfade_samples:
            result = torch.cat([blended, wav[:, crossfade_samples:]], dim=-1)
        else:
            result = blended
        
        return result
    
    def end(self):
        """Called when generation is complete."""
        # Mark any remaining tokens as final chunk
        if self.token_buffer:
            self._mark_chunk_boundary(is_final=True)
            
        if self.config.verbose:
            total_time = time.perf_counter() - self.start_time
            print(f"  [Stream] Generation complete:")
            print(f"    Total tokens: {len(self.all_generated_tokens)}")
            print(f"    Chunks marked: {self.chunk_count}")
            print(f"    Generation time: {total_time:.3f}s")
    
    def get_codes(self) -> torch.Tensor:
        """
        Get the generated mel tokens as a tensor.
        Call this after generation is complete.
        """
        if not self.all_generated_tokens:
            raise ValueError("No tokens generated")
        return torch.tensor([self.all_generated_tokens], dtype=torch.long, device=self.spk_cond_emb.device)


def streaming_inference(
    tts: 'IndexTTS2',
    text: str,
    audio_prompt: Optional[str] = None,
    speaker_embeddings: Optional[dict] = None,
    emotion_audio: Optional[str] = None,
    emotion_alpha: float = 1.0,
    emo_vector: Optional[list] = None,
    use_emo_text: bool = False,
    emo_text: Optional[str] = None,
    use_random: bool = False,
    config: Optional[StreamingConfig] = None,
    # Pattern embedding support
    pattern_embedding: Optional[Any] = None,
    injection_mode: str = "add",
    # Generation parameters
    temperature: float = 0.8,
    top_p: float = 0.8,
    top_k: int = 30,
    max_mel_tokens: int = 600,
    # Callbacks
    on_audio_chunk: Optional[Callable[[torch.Tensor], None]] = None,
) -> Generator[torch.Tensor, None, None]:
    """
    True streaming TTS inference that yields audio chunks as they're generated.
    
    This function intercepts mel tokens from GPT generation and processes
    them in chunks, yielding audio with minimal latency.
    
    Args:
        tts: IndexTTS2 instance
        text: Text to synthesize
        audio_prompt: Path to speaker reference audio
        speaker_embeddings: Pre-computed speaker embeddings (alternative to audio_prompt)
        emotion_audio: Path to emotion reference audio
        emotion_alpha: Emotion mixing weight
        emo_vector: Explicit emotion vector
        use_emo_text: Extract emotion from text
        emo_text: Text for emotion extraction
        use_random: Use random emotion sampling
        config: StreamingConfig instance
        pattern_embedding: PatternEmbedding instance for pattern-aware inference
        injection_mode: How to inject pattern embedding ("add", "prepend", "replace_first")
        temperature: Sampling temperature
        top_p: Top-p sampling threshold
        top_k: Top-k sampling threshold
        max_mel_tokens: Maximum mel tokens to generate
        on_audio_chunk: Callback for each audio chunk
        
    Yields:
        Audio chunks as torch.Tensor (1, samples) at 22050 Hz
    """
    import librosa
    import torchaudio
    import random as rnd
    
    if config is None:
        config = StreamingConfig()
    
    device = tts.device
    if isinstance(device, str):
        device = torch.device(device)
    
    use_autocast = tts.dtype is not None and device.type == 'cuda'
    
    # === CONDITIONING EXTRACTION ===
    if config.verbose:
        print(f"[Streaming] Extracting conditioning...")
        start_time = time.perf_counter()
    
    # Handle emotion settings
    if use_emo_text or emo_vector is not None:
        emotion_audio = None
    
    if use_emo_text:
        if emo_text is None:
            emo_text = text
        emo_dict = tts.qwen_emo.inference(emo_text)
        if config.verbose:
            print(f"  Detected emotions: {emo_dict}")
        emo_vector = list(emo_dict.values())
    
    if emo_vector is not None:
        emo_vector_scale = max(0.0, min(1.0, emotion_alpha))
        if emo_vector_scale != 1.0:
            emo_vector = [x * emo_vector_scale for x in emo_vector]
        emo_vector = tts.normalize_emo_vec(emo_vector)
    
    # === Extract audio conditioning ===
    if audio_prompt is not None:
        audio, sr = librosa.load(audio_prompt, sr=None, mono=True)
        audio = audio[:int(15 * sr)]
        
        audio_tensor = torch.tensor(audio).unsqueeze(0)
        audio_16k = torchaudio.transforms.Resample(sr, 16000)(audio_tensor)
        audio_22k = torchaudio.transforms.Resample(sr, 22050)(audio_tensor)
        
        inputs = tts.extract_features(audio_16k, sampling_rate=16000, return_tensors="pt")
        input_features = inputs["input_features"].to(device)
        attention_mask = inputs["attention_mask"].to(device)
        
        with torch.no_grad():
            with torch.amp.autocast(device.type, enabled=use_autocast, dtype=tts.dtype or torch.float32):
                spk_cond_emb = tts.get_emb(input_features, attention_mask)
                cond_lengths = torch.tensor([spk_cond_emb.shape[1]], device=device)
                speech_conditioning_latent = tts.gpt.get_conditioning(
                    spk_cond_emb.transpose(1, 2), cond_lengths
                )
                
                emo_cond = tts.gpt.get_emo_conditioning(spk_cond_emb.transpose(1, 2), cond_lengths)
                emo_vec = tts.gpt.emovec_layer(emo_cond)
                emo_vec = tts.gpt.emo_layer(emo_vec)
                
                _, S_ref = tts.semantic_codec.quantize(spk_cond_emb)
                ref_mel = tts.mel_fn(audio_22k.to(device).float())
                ref_target_lengths = torch.LongTensor([ref_mel.size(2)]).to(device)
                
                feat = torchaudio.compliance.kaldi.fbank(
                    audio_16k.to(device),
                    num_mel_bins=80,
                    dither=0,
                    sample_frequency=16000
                )
                feat = feat - feat.mean(dim=0, keepdim=True)
                style = tts.campplus_model(feat.unsqueeze(0))
                
                prompt_condition = tts.s2mel.models['length_regulator'](
                    S_ref, ylens=ref_target_lengths, n_quantizers=3, f0=None
                )[0]
    
    elif speaker_embeddings is not None:
        spk_cond_emb = speaker_embeddings['spk_cond_emb'].to(device)
        speech_conditioning_latent = speaker_embeddings.get('gpt_conditioning')
        emo_vec = speaker_embeddings.get('emo_cond_emb', spk_cond_emb).to(device)
        style = speaker_embeddings['style'].to(device)
        prompt_condition = speaker_embeddings['prompt_condition'].to(device)
        ref_mel = speaker_embeddings['ref_mel'].to(device)
        
        if speech_conditioning_latent is None:
            cond_lengths = torch.tensor([spk_cond_emb.shape[1]], device=device)
            with torch.no_grad():
                with torch.amp.autocast(device.type, enabled=use_autocast, dtype=tts.dtype or torch.float32):
                    speech_conditioning_latent = tts.gpt.get_conditioning(
                        spk_cond_emb.transpose(1, 2), cond_lengths
                    )
                    emo_cond = tts.gpt.get_emo_conditioning(spk_cond_emb.transpose(1, 2), cond_lengths)
                    emo_vec = tts.gpt.emovec_layer(emo_cond)
                    emo_vec = tts.gpt.emo_layer(emo_vec)
        else:
            speech_conditioning_latent = speech_conditioning_latent.to(device)
    else:
        raise ValueError("Either audio_prompt or speaker_embeddings must be provided")
    
    # Handle emotion reference audio
    if emotion_audio is not None:
        emo_audio, emo_sr = librosa.load(emotion_audio, sr=None, mono=True)
        emo_audio = emo_audio[:int(15 * emo_sr)]
        emo_audio_16k = librosa.resample(emo_audio, orig_sr=emo_sr, target_sr=16000)
        emo_audio_tensor = torch.from_numpy(emo_audio_16k).unsqueeze(0)
        
        with torch.no_grad():
            with torch.amp.autocast(device.type, enabled=use_autocast, dtype=tts.dtype or torch.float32):
                emo_inputs = tts.extract_features(emo_audio_tensor, sampling_rate=16000, return_tensors="pt")
                emo_input_features = emo_inputs["input_features"].to(device)
                emo_attention_mask = emo_inputs["attention_mask"].to(device)
                emo_emb = tts.get_emb(emo_input_features, emo_attention_mask)
                
                emo_cond_lengths = torch.tensor([emo_emb.shape[1]], device=device)
                new_emo = tts.gpt.get_emo_conditioning(emo_emb.transpose(1, 2), emo_cond_lengths)
                new_emo = tts.gpt.emovec_layer(new_emo)
                new_emo = tts.gpt.emo_layer(new_emo)
                
                emo_vec = emo_vec + emotion_alpha * (new_emo - emo_vec)
    
    # Handle explicit emotion vector
    if emo_vector is not None:
        weight_vector = torch.tensor(emo_vector, device=device)
        
        if use_random:
            random_index = [rnd.randint(0, x - 1) for x in tts.emo_num]
        else:
            def find_most_similar_cosine(query_vector, matrix):
                query_vector = query_vector.float()
                matrix = matrix.float()
                similarities = F.cosine_similarity(query_vector, matrix, dim=1)
                return torch.argmax(similarities)
            
            random_index = [find_most_similar_cosine(style, tmp) for tmp in tts.spk_matrix]
        
        emo_matrix_selected = [tmp[index].unsqueeze(0) for index, tmp in zip(random_index, tts.emo_matrix)]
        emo_matrix_selected = torch.cat(emo_matrix_selected, 0)
        emovec_mat = weight_vector.unsqueeze(1) * emo_matrix_selected
        emovec_mat = torch.sum(emovec_mat, 0)
        emovec_mat = emovec_mat.unsqueeze(0)
        
        weight_sum = sum(emo_vector)
        emo_vec = emovec_mat + (1 - weight_sum) * emo_vec
    
    if config.verbose:
        print(f"  Conditioning extracted in {time.perf_counter() - start_time:.3f}s")
    
    # === TOKENIZE TEXT ===
    text_tokens_list = tts.tokenizer.tokenize(text)
    text_token_ids = tts.tokenizer.convert_tokens_to_ids(text_tokens_list)
    text_tokens = torch.tensor(text_token_ids, dtype=torch.long, device=device).unsqueeze(0)
    
    if config.verbose:
        print(f"[Streaming] Text: {len(text_tokens_list)} tokens")
    
    # === PREPARE GPT INPUTS ===
    batch_size = 1
    use_speed = torch.zeros(batch_size, dtype=torch.long, device=device)
    duration_ctrl = tts.gpt.speed_emb(torch.ones_like(use_speed))
    duration_free = tts.gpt.speed_emb(torch.zeros_like(use_speed))
    
    if emo_vec.dim() == 2:
        emo_vec_expanded = emo_vec.unsqueeze(1)
    else:
        emo_vec_expanded = emo_vec
    
    # === PATTERN EMBEDDING INJECTION ===
    # If pattern embedding is provided, inject it into the conditioning
    final_conditioning = speech_conditioning_latent
    if pattern_embedding is not None:
        if config.verbose:
            print(f"[Streaming] Injecting pattern embedding (mode={injection_mode})")
        with torch.no_grad():
            final_conditioning = pattern_embedding.get_injection_embedding(
                speech_conditioning_latent,
                injection_mode=injection_mode,
            )
        if config.verbose:
            print(f"  Conditioning shape after injection: {final_conditioning.shape}")
    
    conds_latent = torch.cat(
        (final_conditioning + emo_vec_expanded,
         duration_ctrl.unsqueeze(1),
         duration_free.unsqueeze(1)),
        dim=1,
    )
    
    input_ids, inputs_embeds, attention_mask = tts.gpt.prepare_gpt_inputs(conds_latent, text_tokens)
    tts.gpt.inference_model.store_mel_emb(inputs_embeds)
    
    # Ensure 2D emo_vec for GPT forward
    emo_vec_2d = emo_vec.squeeze(1) if emo_vec.dim() == 3 else emo_vec
    
    # === CREATE STREAMER ===
    collected_chunks = []
    
    def chunk_callback(wav_chunk):
        collected_chunks.append(wav_chunk)
    
    # Track emo_cond_emb (same as spk_cond_emb when no separate emotion audio)
    emo_cond_emb = spk_cond_emb
    
    streamer = MelTokenStreamer(
        tts=tts,
        config=config,
        spk_cond_emb=spk_cond_emb,
        emo_cond_emb=emo_cond_emb,  # Pass through for synthesis
        emo_vec=emo_vec_2d,
        text_tokens=text_tokens,
        speech_conditioning_latent=final_conditioning,  # Use pattern-injected conditioning
        style=style,
        prompt_condition=prompt_condition,
        ref_mel=ref_mel,
        on_audio_chunk=chunk_callback if on_audio_chunk is None else on_audio_chunk,
    )
    
    if config.verbose:
        print(f"[Streaming] Starting GPT generation...")
    
    # === STREAMING MODE: Queue-based concurrent generation and synthesis ===
    if config.synthesize_during_generation:
        # Use a queue to yield audio chunks as they're produced
        audio_queue: queue.Queue[Optional[torch.Tensor]] = queue.Queue()
        generation_done = threading.Event()
        generation_error: List[Exception] = []
        
        def on_chunk(wav):
            """Callback when a chunk is synthesized - put it in the queue."""
            audio_queue.put(wav)
        
        # Update streamer to use queue callback
        streamer.on_audio_chunk = on_chunk
        
        def run_generation():
            """Run GPT generation in background thread."""
            try:
                with torch.no_grad():
                    with torch.amp.autocast(device.type, enabled=use_autocast, dtype=tts.dtype or torch.float32):
                        tts.gpt.inference_model.generate(
                            input_ids,
                            bos_token_id=tts.gpt.start_mel_token,
                            pad_token_id=tts.gpt.stop_mel_token,
                            eos_token_id=tts.gpt.stop_mel_token,
                            attention_mask=attention_mask,
                            max_length=input_ids.shape[1] + max_mel_tokens - 1,
                            do_sample=True,
                            top_p=top_p,
                            top_k=top_k,
                            temperature=temperature,
                            num_return_sequences=1,
                            streamer=streamer,
                        )
            except Exception as e:
                generation_error.append(e)
            finally:
                generation_done.set()
                audio_queue.put(None)  # Sentinel to signal end
        
        # Start generation in background
        gen_thread = threading.Thread(target=run_generation, daemon=True)
        gen_start = time.perf_counter()
        gen_thread.start()
        
        # Yield audio chunks as they arrive
        chunk_count = 0
        first_yield_time = None
        while True:
            try:
                # Wait for next chunk with timeout
                wav = audio_queue.get(timeout=0.1)
                if wav is None:
                    # Generation complete
                    break
                chunk_count += 1
                if first_yield_time is None:
                    first_yield_time = time.perf_counter() - gen_start
                    if config.verbose:
                        print(f"  [Yield] First audio chunk at {first_yield_time:.3f}s")
                yield wav
            except queue.Empty:
                # Check if generation is done
                if generation_done.is_set():
                    # Drain remaining items
                    while not audio_queue.empty():
                        wav = audio_queue.get_nowait()
                        if wav is not None:
                            chunk_count += 1
                            yield wav
                    break
        
        # Wait for thread to finish
        gen_thread.join(timeout=5.0)
        
        if generation_error:
            raise generation_error[0]
        
        gen_time = time.perf_counter() - gen_start
        if config.verbose:
            print(f"[Streaming] Generation done in {gen_time:.3f}s")
            print(f"[Streaming] Yielded {chunk_count} chunks")
            if first_yield_time:
                print(f"[Streaming] Time to first audio: {first_yield_time:.3f}s")
    else:
        # === NON-STREAMING MODE: Generate all tokens first, then synthesize ===
        gen_start = time.perf_counter()
        with torch.no_grad():
            with torch.amp.autocast(device.type, enabled=use_autocast, dtype=tts.dtype or torch.float32):
                tts.gpt.inference_model.generate(
                    input_ids,
                    bos_token_id=tts.gpt.start_mel_token,
                    pad_token_id=tts.gpt.stop_mel_token,
                    eos_token_id=tts.gpt.stop_mel_token,
                    attention_mask=attention_mask,
                    max_length=input_ids.shape[1] + max_mel_tokens - 1,
                    do_sample=True,
                    top_p=top_p,
                    top_k=top_k,
                    temperature=temperature,
                    num_return_sequences=1,
                    streamer=streamer,
                )
        
        gen_time = time.perf_counter() - gen_start
        if config.verbose:
            print(f"[Streaming] GPT generation done in {gen_time:.3f}s, {len(streamer.all_generated_tokens)} tokens")
        
        # Synthesize chunks after generation (fallback mode)
        chunks = streamer.pending_chunks
        
        if config.verbose:
            total_tokens = sum(len(c) for c in chunks)
            print(f"[Streaming] Synthesizing {total_tokens} tokens in {len(chunks)} chunks...")
        
        synth_start = time.perf_counter()
        diffusion_steps = config.diffusion_steps
        inference_cfg_rate = config.inference_cfg_rate
        
        # Track previous chunk tail for crossfading
        previous_chunk_tail: Optional[torch.Tensor] = None
        
        for chunk_idx, chunk_tokens in enumerate(chunks):
            chunk_start = time.perf_counter()
            is_first = chunk_idx == 0
            is_final = chunk_idx == len(chunks) - 1
            
            # Convert chunk tokens to tensor
            codes = torch.tensor([chunk_tokens], dtype=torch.long, device=device)
            code_lens = torch.tensor([len(chunk_tokens)], device=device)
            
            with torch.no_grad():
                with torch.amp.autocast(device.type, enabled=use_autocast, dtype=tts.dtype or torch.float32):
                    # GPT forward pass for latent
                    use_speed = torch.zeros(1, device=device, dtype=torch.long)
                    
                    latent = tts.gpt(
                        final_conditioning,  # Use pattern-injected conditioning
                        text_tokens,
                        torch.tensor([text_tokens.shape[-1]], device=device),
                        codes,
                        code_lens,
                        spk_cond_emb,  # emo_cond_mel = spk when no separate emotion audio
                        cond_mel_lengths=torch.tensor([spk_cond_emb.shape[1]], device=device),
                        emo_cond_mel_lengths=torch.tensor([spk_cond_emb.shape[1]], device=device),
                        emo_vec=emo_vec_2d,
                        use_speed=use_speed,
                    )
                    
                    # S2Mel stage
                    latent = tts.s2mel.models['gpt_layer'](latent)
                    S_infer = tts.semantic_codec.quantizer.vq2emb(codes.unsqueeze(1))
                    S_infer = S_infer.transpose(1, 2)
                    S_infer = S_infer + latent
                    target_lengths = (code_lens * 1.72).long()
                    
                    cond = tts.s2mel.models['length_regulator'](
                        S_infer, ylens=target_lengths, n_quantizers=3, f0=None
                    )[0]
                    
                    cat_condition = torch.cat([prompt_condition, cond], dim=1)
                    
                    # Diffusion with fewer steps for first/middle chunks, full for final
                    if is_first:
                        chunk_diffusion_steps = config.first_chunk_diffusion_steps
                    elif is_final:
                        chunk_diffusion_steps = diffusion_steps
                    else:
                        chunk_diffusion_steps = max(8, diffusion_steps - 5)
                    
                    vc_target = tts.s2mel.models['cfm'].inference(
                        cat_condition,
                        torch.LongTensor([cat_condition.size(1)]).to(device),
                        ref_mel,
                        style,
                        None,
                        chunk_diffusion_steps,
                        inference_cfg_rate=inference_cfg_rate
                    )
                    vc_target = vc_target[:, :, ref_mel.size(-1):]
                    
                    # BigVGAN vocoding
                    wav = tts.bigvgan(vc_target.float()).squeeze()
            
            # Clamp like original
            wav = torch.clamp(32767 * wav, -32767.0, 32767.0).cpu()
            if wav.dim() == 1:
                wav = wav.unsqueeze(0)
            
            # Apply crossfade with previous chunk for smooth transitions
            if config.use_crossfade and previous_chunk_tail is not None:
                crossfade_samples = min(
                    config.crossfade_samples,
                    previous_chunk_tail.shape[-1],
                    wav.shape[-1]
                )
                
                if crossfade_samples >= 32:  # Only if meaningful
                    # Use raised cosine for smoother crossfade if enabled
                    if getattr(config, 'use_cosine_crossfade', False):
                        t = torch.linspace(0, 1, crossfade_samples, device=wav.device, dtype=wav.dtype)
                        fade_in = 0.5 * (1 - torch.cos(torch.pi * t))
                        fade_out = 0.5 * (1 + torch.cos(torch.pi * t))
                    else:
                        # Linear crossfade
                        fade_in = torch.linspace(0.0, 1.0, crossfade_samples, device=wav.device, dtype=wav.dtype)
                        fade_out = torch.linspace(1.0, 0.0, crossfade_samples, device=wav.device, dtype=wav.dtype)
                    
                    # Get overlapping regions
                    prev_tail = previous_chunk_tail[:, -crossfade_samples:]
                    curr_head = wav[:, :crossfade_samples]
                    
                    # Blend
                    blended = (prev_tail * fade_out.unsqueeze(0)) + (curr_head * fade_in.unsqueeze(0))
                    
                    # Create output: blended region + rest of current chunk
                    if wav.shape[-1] > crossfade_samples:
                        wav = torch.cat([blended, wav[:, crossfade_samples:]], dim=-1)
                    else:
                        wav = blended
            
            # For non-final chunks: save tail for blending and TRIM it from output
            # This prevents echo/bleeding - the tail is only heard in the blend, not twice
            if config.use_crossfade and not is_final:
                crossfade_samples = config.crossfade_samples
                if wav.shape[-1] > crossfade_samples * 2:
                    # Store tail for blending into next chunk
                    previous_chunk_tail = wav[:, -crossfade_samples:].clone()
                    # TRIM tail from current output - it will appear in next chunk's blend
                    wav = wav[:, :-crossfade_samples]
                else:
                    previous_chunk_tail = None
            else:
                # Final chunk - output everything, no tail to store
                previous_chunk_tail = None
            
            chunk_time = time.perf_counter() - chunk_start
            if config.verbose:
                print(f"  [Chunk {chunk_idx+1}/{len(chunks)}] {len(chunk_tokens)} tokens -> {wav.shape[-1]} samples in {chunk_time:.3f}s")
            
            if on_audio_chunk is not None:
                on_audio_chunk(wav)
            yield wav
        
        if config.verbose:
            synth_time = time.perf_counter() - synth_start
            total_time = time.perf_counter() - start_time
            print(f"[Streaming] Synthesis done in {synth_time:.3f}s")
            print(f"[Streaming] Total time: {total_time:.3f}s")


def streaming_inference_generator(
    tts: 'IndexTTS2',
    text: str,
    audio_prompt: Optional[str] = None,
    speaker_embeddings: Optional[dict] = None,
    pattern_embedding: Optional[Any] = None,
    config: Optional[StreamingConfig] = None,
    **kwargs
) -> Generator[torch.Tensor, None, None]:
    """
    Convenience wrapper for streaming_inference that returns a generator.
    
    This is the main entry point for streaming TTS synthesis.
    
    Args:
        tts: IndexTTS2 instance
        text: Text to synthesize
        audio_prompt: Path to speaker reference audio
        speaker_embeddings: Pre-computed speaker embeddings
        pattern_embedding: PatternEmbedding instance for pattern-aware inference
        config: StreamingConfig instance
        **kwargs: Additional arguments passed to streaming_inference
    """
    # Check if HIGH quality mode is requested - use V2 backend for best quality
    if config is not None and config.quality == StreamingQuality.HIGH:
        try:
            from indextts.streaming_v2 import streaming_inference_v2, StreamingConfigV2, StreamingMode
            
            # Convert to V2 config
            v2_config = StreamingConfigV2(
                mode=StreamingMode.SENTENCE_LEVEL,
                diffusion_steps=config.diffusion_steps,
                first_chunk_diffusion_steps=config.first_chunk_diffusion_steps,
                inference_cfg_rate=config.inference_cfg_rate,
                crossfade_samples=config.crossfade_samples,
                verbose=config.verbose,
            )
            
            yield from streaming_inference_v2(
                tts=tts,
                text=text,
                audio_prompt=audio_prompt,
                speaker_embeddings=speaker_embeddings,
                pattern_embedding=pattern_embedding,
                config=v2_config,
                **kwargs
            )
            return
        except ImportError:
            # Fall back to V1 if V2 not available
            pass
    
    yield from streaming_inference(
        tts=tts,
        text=text,
        audio_prompt=audio_prompt,
        speaker_embeddings=speaker_embeddings,
        pattern_embedding=pattern_embedding,
        config=config,
        **kwargs
    )


# === CONVENIENCE FUNCTIONS FOR QUALITY PRESETS ===

def get_fast_config() -> StreamingConfig:
    """Get configuration optimized for lowest latency (fastest TTFA)."""
    return StreamingConfig(quality=StreamingQuality.FAST)


def get_balanced_config() -> StreamingConfig:
    """Get balanced configuration (recommended default)."""
    return StreamingConfig(quality=StreamingQuality.BALANCED)


def get_high_quality_config() -> StreamingConfig:
    """Get configuration optimized for best audio quality (uses V2 sentence-level streaming)."""
    return StreamingConfig(quality=StreamingQuality.HIGH)