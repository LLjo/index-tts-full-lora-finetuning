# LoRA Fine-tuning Configuration for IndexTTS Voice Adaptation
#
# This configuration is optimized for voice adaptation using LoRA.
# Adjust parameters based on your dataset size and computational resources.

# Dataset Configuration
dataset:
  # Paths to manifests generated by tools/prepare_lora_dataset.py
  train_manifest: "data/my_voice_processed/train_manifest.jsonl"
  val_manifest: "data/my_voice_processed/val_manifest.jsonl"
  
  # Train/validation split (if not using pre-split manifests)
  train_split: 0.9

# Model Configuration
model:
  # Base model checkpoint to load
  base_checkpoint: "checkpoints/gpt.pth"
  config: "checkpoints/config.yaml"
  tokenizer: "checkpoints/bpe.model"

# LoRA Hyperparameters
lora:
  # LoRA rank: higher = more capacity, slower training, larger checkpoints
  # Typical values: 4-16
  # - Use 4-8 for small datasets (50-200 samples)
  # - Use 8-12 for medium datasets (200-500 samples)
  # - Use 12-16 for large datasets (500+ samples)
  rank: 8
  
  # LoRA alpha: scaling factor, usually 2*rank
  alpha: 16
  
  # Dropout for LoRA layers (regularization)
  dropout: 0.05
  
  # Whether to apply LoRA to embedding layers (stronger adaptation)
  include_embeddings: false
  
  # Whether to apply LoRA to output heads
  include_heads: true

# Training Hyperparameters
training:
  # Output directory for LoRA checkpoints
  output_dir: "trained_lora/my_voice"
  
  # Batch size (adjust based on GPU memory)
  # - 4-8 for GPUs with 8-12GB VRAM
  # - 8-16 for GPUs with 16-24GB VRAM
  batch_size: 8
  
  # Gradient accumulation steps (effective_batch = batch_size * grad_accumulation)
  grad_accumulation: 2
  
  # Number of training epochs
  # LoRA trains faster than full fine-tuning, so fewer epochs needed
  # - 10-20 epochs for small datasets
  # - 5-15 epochs for medium/large datasets
  epochs: 20
  
  # Learning rate (higher than full fine-tuning)
  # Typical range: 1e-4 to 5e-4
  learning_rate: 3e-4
  
  # Weight decay for regularization
  weight_decay: 0.01
  
  # Warmup steps for learning rate scheduler
  warmup_steps: 100
  
  # Maximum training steps (0 = unlimited, determined by epochs)
  max_steps: 0
  
  # Gradient clipping to prevent exploding gradients
  grad_clip: 1.0
  
  # Mixed precision training (recommended for faster training)
  amp: true
  
  # Loss weights
  text_loss_weight: 0.2
  mel_loss_weight: 0.8
  
  # Logging and validation
  log_interval: 50        # Log training metrics every N steps
  val_interval: 0         # Validate every N steps (0 = end of epoch)
  save_interval: 500      # Save checkpoint every N steps
  
  # DataLoader workers (0 = main process, 2-4 recommended for speed)
  num_workers: 0
  
  # Random seed for reproducibility
  seed: 1234

# Hardware Configuration
hardware:
  # Device to use (auto-detected if not specified)
  # Options: "cuda", "cuda:0", "cpu", "mps", "xpu"
  device: null  # null = auto-detect
  
  # Number of GPUs to use (for future multi-GPU support)
  num_gpus: 1

# Notes:
# - Start with these default values
# - If training loss doesn't decrease: increase learning_rate or lora_rank
# - If overfitting (val loss increases): decrease lora_rank, increase dropout, or reduce epochs
# - For faster convergence: increase batch_size and learning_rate together
# - Monitor GPU memory usage and adjust batch_size accordingly